{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DateType, TimestampType, IntegerType\n",
    "import seaborn as sns\n",
    "import d5_solutions as sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def preprocess_cdrs_using_spark(file_or_folder=None, number_of_users_to_sample=None,\n",
    "                                output_csv=None, date_format='%Y%m%d%H%M%S',\n",
    "                                debug_mode=True, loc_file=None, save_to_csv=False):\n",
    "    \"\"\"\n",
    "    In this function, we perfom some basic preprocessing such as below:\n",
    "    1. rename columns\n",
    "    2. change some data types\n",
    "    3. Add location details\n",
    "    Eventually, we will sample the data to use for our analysis\n",
    "    :param data_folder:\n",
    "    :param output_csv_for_sample_users:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # create SparkSession object\n",
    "    spark = SparkSession.builder.master(\"local[8]\").appName(\"data_processor\").getOrCreate()\n",
    "\n",
    "    # read data with spark\n",
    "    df = spark.read.csv(path=file_or_folder, header=True)\n",
    "\n",
    "    # repartition to speed up\n",
    "    df = df.repartition(10)\n",
    "\n",
    "    # if just testing/debugging, pick only a small dataset\n",
    "    if debug_mode:\n",
    "        dfs = df.sample(fraction=0.01)\n",
    "        df = dfs\n",
    "\n",
    "    # rename columns to remove space and replace with underscore\n",
    "    df2 = (df.withColumnRenamed(\"cdr datetime\", \"cdrDatetime\")\n",
    "        .withColumnRenamed(\"calling phonenumber2\", \"phoneNumber\")\n",
    "        .withColumnRenamed(\"last calling cellid\", \"cellId\")\n",
    "        .withColumnRenamed(\"call duration\", \"cellDuration\"))\n",
    "\n",
    "    # drop cdr type column\n",
    "    df3 = df2.drop('cdr type')\n",
    "\n",
    "    # Use Spark UDF to add date and datetime\n",
    "    add_datetime = udf(lambda x: datetime.strptime(x, date_format), TimestampType())\n",
    "    add_date = udf(lambda x: datetime.strptime(x, date_format), DateType())\n",
    "\n",
    "    # create timestamp\n",
    "    df4 = df3.withColumn('datetime', add_datetime(col('cdrDatetime')))\n",
    "    df5 = df4.withColumn('date', add_date(col('cdrDatetime')))\n",
    "\n",
    "    # lets make sure we dont have any null phoneNumbers\n",
    "    df6 = df5.filter(df5['phoneNumber'].isNotNull())\n",
    "\n",
    "    # Lets merge with location details using cellId from CDRs and also\n",
    "    # cellID on the other\n",
    "    dfLoc = pd.read_csv(loc_file)\n",
    "    dfLoc.rename(columns={'cell_id': 'cellId'}, inplace=True)\n",
    "    sdfLoc = spark.createDataFrame(dfLoc)\n",
    "    df7 = df6.join(sdfLoc, on='cellId', how='inner')\n",
    "    \n",
    "    return df7\n",
    "\n",
    "#     # select nsample users to work with\n",
    "#     all_users = df7.select('phoneNumber').distinct().collect()\n",
    "\n",
    "#     # randomly select users using filter statement\n",
    "#     random_user_numbers = [i['phoneNumber'] for i in random.choices(all_users, k=number_of_users_to_sample)]\n",
    "\n",
    "#     # select only our random user data\n",
    "#     dfu = df7.filter(df7['phoneNumber'].isin(random_user_numbers))\n",
    "\n",
    "#     # save to CSV if necessary\n",
    "#     if save_to_csv:\n",
    "#         dfu.coalesce(1).write.csv(path=output_csv, header=True)\n",
    "#     else:\n",
    "#         return dfu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def combine_selected_csv_files(folder_with_csv_files=None, number_to_save=None, out_csv_file=None):\n",
    "    \"\"\"\n",
    "    Save a sample of the small CSV files into a CSV file for exploration.\n",
    "    Please test this with very few files to avoid wasting time\n",
    "    :param folder_with_csv_files:\n",
    "    :param number_to_save:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # get a list of CSV file using os module listdir() function\n",
    "    files = os.listdir(folder_with_csv_files)\n",
    "    \n",
    "    # create a list to hold pandas dataframes\n",
    "    df_lst = []\n",
    "    \n",
    "    #create a counter variable whcih will help you stop\n",
    "    # the loop when you reach the required number of files\n",
    "    cnt = 0\n",
    "    for f in files:\n",
    "        if f.endswith('csv'):\n",
    "            fpath = os.path.join(folder_with_csv_files, f)\n",
    "            df = pd.read_csv(fpath)\n",
    "            # append this df to the list of dfs above\n",
    "            df_lst.append(df)\n",
    "    \n",
    "            # increment the counter variable\n",
    "            cnt += 1\n",
    "    \n",
    "            # stop the loop using break statement when you have\n",
    "            # processes the required number of files\n",
    "            # as defined by number_to_save\n",
    "            if cnt == number_to_save:\n",
    "                break\n",
    "    \n",
    "    # use pandas function concat() like this: pd.concat()\n",
    "    # to concatenate all the dfs in the list\n",
    "    df = pd.concat(df_lst)\n",
    "    \n",
    "    # save your new dataframe\n",
    "    df.to_csv(out_csv_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def explore_data(df=None, output_plot_file=None, output_heatmap=None):\n",
    "    \"\"\"\n",
    "    For quick examination of user activity, lets generate\n",
    "    user call count and do a simple plot.\n",
    "    \"\"\"\n",
    "    # Number of days in data\n",
    "    dates_rows = df.select('date').distinct().collect()\n",
    "    sorted_dates = sorted([i['date'] for i in dates_rows])\n",
    "    diff = sorted_dates[-1] - sorted_dates[0]\n",
    "    num_days = diff.days\n",
    "\n",
    "    # call count by hour\n",
    "    add_hr = udf(lambda x: x.hour, IntegerType())\n",
    "    add_wkday = udf(lambda x: x.weekday(), IntegerType())\n",
    "    day_dict = {0: 'Mon', '1': 'Tue', '2': 'Wed', 3: 'Thurs', 4: 'Frid', 5: 'Sat', 6: 'Sun'}\n",
    "\n",
    "    dfHr = df.withColumn('hr', add_hr(col('datetime')))\n",
    "    dfHr2 = dfHr.withColumn('wkday', add_wkday(col('datetime')))\n",
    "    dfWkDay = dfHr2.groupBy('wkday', 'hr').count().toPandas()\n",
    "    dfWkDay['weekDay'] = dfWkDay.apply(add_weekdays, args=(day_dict,), axis=1)\n",
    "    dfWkDay.drop(labels=['wkday'], axis=1, inplace=True)\n",
    "    dfWkDayPivot = dfWkDay.pivot(index='weekDay', columns='hr', values='count')\n",
    "    d = dfWkDayPivot.reset_index()\n",
    "    ax = sns.heatmap(d)\n",
    "    ax.get_figure().savefig(output_heatmap)\n",
    "\n",
    "    # group user and count number of events\n",
    "    # convert resulting spark dataframe to pandas\n",
    "    dfGroup = df.groupBy('phoneNumber').count().toPandas()\n",
    "\n",
    "    # create a distribution plot of user call count using\n",
    "    # seaborn\n",
    "    ax = sns.distplot(dfGroup['count'])\n",
    "\n",
    "    # save plot as png file\n",
    "    ax.get_figure().savefig(output_plot_file)\n",
    "\n",
    "    # report average number calls per day for each user\n",
    "    dfGroupDay = df.groupBy('phoneNumber', 'date').count().toPandas()\n",
    "\n",
    "    # get mean and median\n",
    "    mean = dfGroupDay['count'].mean()\n",
    "    median = dfGroupDay['count'].median()\n",
    "\n",
    "    # data duration\n",
    "    return mean, median, num_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFolder = '../../day5-case-study/cdrs/'\n",
    "users_call_cnt_plt = '../../day5-case-study/userCallCnt.png'\n",
    "heatMap = '../../day5-case-study/dayHrHeatmap.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = preprocess_cdrs_using_spark(file_or_folder=dataFolder, number_of_users_to_sample=50000, \n",
    "                                  date_format='%Y%m%d%H%M%S', \n",
    "                                  loc_file='../../day5-case-study/cellTowers/staggered-cell-locs.csv',\n",
    "                                 save_to_csv=False, debug_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, median, num_days, hrDayCnts = sol.explore_data(df=dfs, output_plot_file=users_call_cnt_plt , \n",
    "             output_heatmap=heatMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(cellId='20532.0', cdrDatetime='20180711204136', cellDuration=None, phoneNumber='8204330690229196471', datetime=datetime.datetime(2018, 7, 11, 20, 41, 36), date=datetime.date(2018, 7, 11), site_id='s91', lon=28.885225, lat=-19.060772)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('8204330690229196471',\n",
       " Row(cellId='20532.0', cdrDatetime='20180711204136', cellDuration=None, phoneNumber='8204330690229196471', datetime=datetime.datetime(2018, 7, 11, 20, 41, 36), date=datetime.date(2018, 7, 11), site_id='s91', lon=28.885225, lat=-19.060772))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grpByUser = dfs.rdd.map(lambda x: (x['phoneNumber'], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('217873087185206285', <pyspark.resultiterable.ResultIterable at 0x1a30c7e828>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grpByUser2 = grpByUser.groupByKey()\n",
    "grpByUser2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpByUser = dfs.rdd.map(lambda x: (x['phoneNumber'], x))\n",
    "grpByUser2 = grpByUser.groupByKey()\n",
    "\n",
    "# select nsample users to work with\n",
    "all_users = df7.select('phoneNumber').distinct().collect()\n",
    "\n",
    "# randomly select users using filter statement\n",
    "random_user_numbers = [i['phoneNumber'] for i in random.choices(all_users, k=number_of_users_to_sample)]\n",
    "\n",
    "# select only our random user data\n",
    "\n",
    "dfu = df7.filter(df7['phoneNumber'].isin(random_user_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select nsample users to work with\n",
    "all_users = dfs.select('phoneNumber').distinct().collect()\n",
    "\n",
    "# randomly select users using filter statement\n",
    "random_user_numbers = [i['phoneNumber'] for i in random.choices(all_users, k=100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfu = grpByUser2.filter(lambda x: x[0] in random_user_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('5030518197241310850',\n",
       " <pyspark.resultiterable.ResultIterable at 0x1a421a5198>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfu.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = dfu.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
