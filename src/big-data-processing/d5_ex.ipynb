{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DateType, TimestampType, IntegerType\n",
    "import seaborn as sns\n",
    "import d5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/dmatekenya/Google-Drive/gigs/aims-dakar-2019/src/day3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weekdays(row, day_dict=None):\n",
    "    \n",
    "    return day_dict[row['wkday']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def preprocess_cdrs_using_spark(file_or_folder=None, number_of_users_to_sample=None,\n",
    "                                output_csv=None, date_format='%Y%m%d%H%M%S', \n",
    "                                debug_mode=True, loc_file=None, save_to_csv=False ):\n",
    "    \"\"\"\n",
    "    In this function, we perfom some basic preprocessing such as below:\n",
    "    1. rename columns\n",
    "    2. change some data types\n",
    "    3. Add location details\n",
    "    Eventually, we will sample the data to use for our analysis\n",
    "    :param data_folder:\n",
    "    :param output_csv_for_sample_users:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # create SparkSession object\n",
    "    spark = SparkSession.builder.master(\"local[8]\").appName(\"data_processor\").getOrCreate()\n",
    "\n",
    "    # read data with spark\n",
    "    df = spark.read.csv(path=file_or_folder, header=True)\n",
    "    \n",
    "    # repartition to speed up \n",
    "    df = df.repartition(10)\n",
    "    \n",
    "    # if just testing/debugging, pick only a small dataset\n",
    "    if debug_mode:\n",
    "        dfs = df.sample(fraction = 0.001)\n",
    "        df = dfs\n",
    "    \n",
    "\n",
    "    # rename columns to remove space and replace with underscore\n",
    "    df2 = (df.withColumnRenamed(\"cdr datetime\", \"cdrDatetime\")\n",
    "             .withColumnRenamed(\"calling phonenumber2\", \"phoneNumber\")\n",
    "             .withColumnRenamed(\"last calling cellid\", \"cellId\")\n",
    "             .withColumnRenamed(\"call duration\", \"cellDuration\"))\n",
    "\n",
    "    # drop cdr type column\n",
    "    df3 = df2.drop('cdr type')\n",
    "\n",
    "    # Use Spark UDF to add date and datetime\n",
    "    add_datetime = udf(lambda x: datetime.strptime(x, date_format), TimestampType())\n",
    "    add_date = udf(lambda x: datetime.strptime(x, date_format), DateType())\n",
    "\n",
    "    # create timestamp\n",
    "    df4 = df3.withColumn('datetime', add_datetime(col('cdrDatetime')))\n",
    "    df5 = df4.withColumn('date', add_date(col('cdrDatetime')))\n",
    "\n",
    "    # lets make sure we dont have any null phoneNumbers\n",
    "    df6 = df5.filter(df5['phoneNumber'].isNotNull())\n",
    "    \n",
    "    # Lets merge with location details using cellId from CDRs and also\n",
    "    # cellID on the other \n",
    "    dfLoc = pd.read_csv(loc_file)\n",
    "    dfLoc.rename(columns = {'cell_id':'cellId'}, inplace=True)\n",
    "    sdfLoc = spark.createDataFrame(dfLoc)\n",
    "    df7 = df6.join(sdfLoc, on='cellId', how='inner')\n",
    "    \n",
    "    \n",
    "    # select nsample users to work with\n",
    "    all_users = df7.select('phoneNumber').distinct().collect()\n",
    "    \n",
    "    # randomly select users using filter statement\n",
    "    random_user_numbers = [i['phoneNumber'] for i in random.choices(all_users, k=100)]\n",
    "    \n",
    "    # select only our random user data\n",
    "    dfu = df7.filter(df7['phoneNumber'].isin(random_user_numbers))\n",
    "    \n",
    "    # save to CSV if necessary\n",
    "    if save_to_csv:\n",
    "        dfu.coalesce(1).write.csv(path=output_csv, header=True)\n",
    "    else:\n",
    "        return dfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def explore_data(df=None, output_plot_file=None, output_heatmap=None):\n",
    "    \"\"\"\n",
    "    For quick examination of user activity, lets generate\n",
    "    user call count and do a simple plot.\n",
    "    \"\"\"\n",
    "    # Number of days in data\n",
    "    dates_rows = dfs.select('date').distinct().collect()\n",
    "    sorted_dates = sorted([i['date'] for i in dates_rows])\n",
    "    diff = sorted_dates[-1] - sorted_dates[0]\n",
    "    num_days = diff.days\n",
    "    \n",
    "    # call count by hour\n",
    "    add_hr = udf(lambda x: x.hour, IntegerType())\n",
    "    add_wkday = udf(lambda x: x.weekday(), IntegerType())\n",
    "    day_dict = {0:'Mon', '1':'Tue', '2':'Wed', 3: 'Thurs', 4:'Frid', 5:'Sat', 6:'Sun'}\n",
    "\n",
    "    dfHr = dfs.withColumn('hr', add_hr(col('datetime')))\n",
    "    dfHr2 = dfHr.withColumn('wkday', add_wkday(col('datetime')))\n",
    "    dfWkDay = dfHr2.groupBy('wkday', 'hr').count().toPandas()\n",
    "    dfWkDay['weekDay'] = dfWkDay.apply(add_weekdays, args=(day_dict,), axis=1)\n",
    "    dfWkDay.drop(labels=['wkday'], axis=1, inplace=True)\n",
    "    dfWkDayPivot = dfWkDay.pivot(index='weekDay', columns='hr', values='count')\n",
    "    d = dfWkDayPivot.reset_index()\n",
    "    ax = sns.heatmap(d)\n",
    "    ax.get_figure().savefig(output_heatmap)\n",
    "   \n",
    "    # group user and count number of events\n",
    "    # convert resulting spark dataframe to pandas\n",
    "    dfGroup = df.groupBy('phoneNumber').count().toPandas()\n",
    "    \n",
    "    # create a distribution plot of user call count using\n",
    "    # seaborn\n",
    "    ax = sns.distplot(dfGroup['count'])\n",
    "\n",
    "    # save plot as png file\n",
    "    ax.get_figure().savefig(output_plot_file)\n",
    "    \n",
    "    # report average number calls per day for each user\n",
    "    dfGroupDay = df.groupBy('phoneNumber', 'date').count().toPandas()\n",
    "    \n",
    "    # get mean and median\n",
    "    mean = dfGroupDay['count'].mean()\n",
    "    median = dfGroupDay['count'].median()\n",
    "    \n",
    "    # data duration\n",
    "    return mean, median                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFolder = '../../day5-case-studies/cdrs-test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = preprocess_cdrs_using_spark(file_or_folder=dataFolder, number_of_users_to_sample=1000, \n",
    "                                  date_format='%Y%m%d%H%M%S', \n",
    "                                  loc_file='../../day5-case-studies/cellTowers/staggered-cell-locs.csv',\n",
    "                                 save_to_csv=False, debug_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfGroup = dfs.groupBy('phoneNumber').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean, med, wk = explore_user_call_count(df=dfs, \n",
    "                                    output_plot_file='../../day5-case-studies/test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rows = dfs.select('date').distinct().collect()\n",
    "sorted_dates = sorted([i['date'] for i in dates_rows])\n",
    "diff = sorted_dates[-1] - sorted_dates[0]\n",
    "diff.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_hr = udf(lambda x: x.hour, IntegerType())\n",
    "add_wkday = udf(lambda x: x.weekday(), IntegerType())\n",
    "day_dict = {0:'Mon', '1':'Tue', '2':'Wed', 3: 'Thurs', 4:'Frid', 5:'Sat', 6:'Sun'}\n",
    "    \n",
    "dfHr = dfs.withColumn('hr', add_hr(col('datetime')))\n",
    "dfHr2 = dfHr.withColumn('wkday', add_wkday(col('datetime')))\n",
    "dfWkDay = dfHr2.groupBy('wkday', 'hr').count().toPandas()\n",
    "dfWkDay['weekDay'] = dfWkDay.apply(add_weekdays, args=(day_dict,), axis=1)\n",
    "dfWkDay.drop(labels=['wkday'], axis=1, inplace=True)\n",
    "dfWkDayPivot = dfWkDay.pivot(index='weekDay', columns='hr', values='count')\n",
    "d = dfWkDayPivot.reset_index()\n",
    "ax = sns.heatmap(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWkDay.drop(labels=['wkday'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dfWkDay.pivot(index='weekDay', columns='hr', values='count')\n",
    "d.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWkDay[dfWkDay['weekDay']== 'Sun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
